{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file comprises of all the required specifications that might be found missing in the actual project notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics used to measure the performance of a model or result are clearly defined. Metrics are justified based on the characteristics of the problem.\n",
    "Which are the metrics?  \n",
    "How the metrics work? (theory, formulas...)  \n",
    "Why they are appropriate based on the characteristics of your problem?  \n",
    " \n",
    "- Having worked on London Bourough-level Crime Analysis, one of the motive of this project is to provide feasible recommendations based on the predictions obtained from the model. Given the number of features in the dataset, I found the K-Means Clustering algorithm to be optimal.  \n",
    "- This involves assigning examples to clusters in an effort to minimize the variance within each cluster.  \n",
    "- The technique is described here: [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering).  \n",
    "- It is implemented via the [KMeans class](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and the main configuration to tune is the “n_clusters” hyperparameter set to the estimated number of clusters in the data.  \n",
    "- Running the London Borough Crime dataset fits the model on the training dataset and predicts a cluster for each example in the dataset. A scatter plot is then created with points colored by their assigned cluster.  \n",
    "- In this case, a reasonable grouping is found, although the unequal equal variance in each dimension makes the method less suited to this dataset.  \n",
    "- After onducting K-Means Clustering to group the boroughs based on the recreational, convenience facilities the results possess as many as 175 unique categories.  \n",
    "- I have used One-hot encoding to have the features and labels in the numerical format: `onehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")`  \n",
    "- The top 5 venues are generated using the `venue` and `freq` after performing one-hot encoding:  \n",
    "\n",
    "`num_top_venues = 5`  \n",
    "\n",
    "`for hood in grouped['BoroughName']:  \n",
    "    print(\"----\"+hood+\"----\")  \n",
    "    temp = grouped[grouped['BoroughName'] == hood].T.reset_index()  \n",
    "    temp.columns = ['venue','freq']  \n",
    "    temp = temp.iloc[1:]  \n",
    "    temp['freq'] = temp['freq'].astype(float)  \n",
    "    temp = temp.round({'freq': 2})  \n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))  \n",
    "    print('\\n')`  \n",
    " - Final results also include metrics like `Safety` and `Score` (Safety Score). In our case, `CrimeToPop` (Recorded crime per 1000 people) as our safety score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem which needs to be solved is clearly defined. A strategy for solving the problem, including discussion of the expected solution, has been made.\n",
    "\n",
    "### Problem Definition\n",
    "What main machine learning task is this problem related to (classification, regression, clustering, etc.)?  \n",
    "What is the input data format (is it a table, text, images, etc.)?  \n",
    "What is the expected output (a single value categorical value, a numeric value, a set of clusters, etc.)?  \n",
    "\n",
    "**The main objective and problem that needs to be addressed:**  \n",
    "In this project, we will study in detail the segmenting, clustering and classification of London Boroughs using Metropolitan Police Service data and Foursquare Developer API. As the city grows and develops, it becomes increasingly important to examine and understand it quantitiatively. The MPS provides open data for Developers, Investors, Policy Makers, City Planners who possess an interest in answering the following questions for development and safety of residents:\n",
    "\n",
    "- What neighbourhoods have the highest crime?\n",
    "- Is population density correlated to crime level?\n",
    "- Using Foursquare data, what venues are most common in different locations within the city?\n",
    "- Does London Datastore provide with specific enough or thick enough data to empower decisions to be made? Or is it too aggregate to provide value in its current detail? Let's find out.\n",
    "\n",
    "The input data is in the text (.csv) format. [MPS Borough Level Crime Data](https://data.london.gov.uk/dataset/recorded_crime_summary) counts the number of crimes in London at the borough-level per month, based on the crime type.\n",
    "\n",
    "The data is available in two files for each level of geography - the most up to date data covering the last available 24 months only and one covering all historic full calendar years. To analyze the most recent patterns, I opted to explore the one with the last available 24 months.\n",
    "\n",
    "In March 2019, the Metropolitan Police Service started to provide offences grouped by the updated Home Office crime classifications. This currently only covers the most recent 24 months of data.\n",
    "\n",
    "\n",
    "\n",
    "### Strategy\n",
    "The main advantage of using K-Means Clustering is the fact that K-means clustering algorithm is used to find groups which have not been explicitly labeled in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets. Based on our business problem i.e., crime analysis and predictions, knowing and having access to the most remote and unknown terrains in the neighborhoods is pivotal to the lawmakers and policy makers to be proactive in their strategy.  \n",
    "\n",
    "The post Clustering phase includes the Segemnating and Classification of clusters i.e., london neighborhoods based on the metrics including `Safety`, `Score` (Safety Score), `CrimeToPop` (Recorded crime per 1000 people). This provides a comprehensive yet simple-to-present method of provoding feasible predictions and recommendations to the policy makers for any necessary changes that needs to be addressed in their existing strategy or approach. The less technical the final presentation or metrics are, the easier it is for the target audience and citizens to comprehend the final results of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "If a dataset is present, features and calculated statistics relevant to the problem have been reported and discussed, along with a sampling of the data. In lieu of a dataset, a thorough description of the input space or input data has been made. Abnormalities or characteristics of the data or input that need to be addressed have been identified.\n",
    "\n",
    "**The nature of the dataset and how it was obtained:**  \n",
    "The input data is in the text (.csv) format. [MPS Borough Level Crime Data](https://data.london.gov.uk/dataset/recorded_crime_summary) counts the number of crimes in London at the borough-level per month, based on the crime type.\n",
    "\n",
    "The data is available in two files for each level of geography - the most up to date data covering the last available 24 months only and one covering all historic full calendar years. To analyze the most recent patterns, I opted to explore the one with the last available 24 months.\n",
    "\n",
    "In March 2019, the Metropolitan Police Service started to provide offences grouped by the updated Home Office crime classifications. This currently only covers the most recent 24 months of data.\n",
    "\n",
    "Below is a list of the crime types covered under the new HO categories:\n",
    "\n",
    "Major Category - Minor Category:\n",
    "\n",
    "- Arson and Criminal Damage - Arson / Criminal Damage\n",
    "- Burglary: Burglary - Business and Community / Burglary - Residential**\n",
    "- Drug Offences: Drug Trafficking / Possession of Drugs\n",
    "- Miscellaneous Crimes Against Society: Absconding from Lawful Custody / Bail Offences / Bigamy / Concealing an Infant Death Close to Birth / Dangerous Driving / Disclosure, Obstruction, False or Misleading State / Exploitation of Prostitution / Forgery or Use of Drug Prescription / Fraud or Forgery Associated with Driver Records / Going Equipped for Stealing / Handling Stolen Goods / Making, Supplying or Possessing Articles for use i / Obscene Publications / Offender Management Act / Other Forgery / Other Notifiable Offences / Perjury / Perverting Course of Justice / Possession of False Documents / Profitting From or Concealing Proceeds of Crime / Soliciting for Prostitution / Threat or Possession With Intent to Commit Crimina / Wildlife Crime\n",
    "- Possession of Weapons: Other Firearm Offences / Possession of Firearm with Intent / Possession of Firearms Offences / Possession of Other Weapon / Possession of Article with Blade or Point\n",
    "- Public Order Offences: Other Offences Against the State, or Public Order / Public Fear Alarm or Distress / Racially or Religiously Aggravated Public Fear / Violent Disorder\n",
    "- Robbery: Robbery of Business Property / Robbery of Personal Property\n",
    "- Sexual Offences*: Other Sexual Offences / Rape\n",
    "- Theft: Bicycle Theft / Other Theft / Shoplifting / Theft from Person\n",
    "- Vehicle Offences: Aggravated Vehicle Taking / Interfering with a Motor Vehicle / Theft from a Motor Vehicle / Theft or Taking of a Motor Vehicle\n",
    "- Violence Against the Person: Homicide / Violence with Injury / Violence without Injury  \n",
    "\n",
    "**Characteristics and features that were addressed dutng the Exploratory Data Analysis and Data Wrangling**\n",
    "a. In addition to the main motive of this project, I also want to provide feasible recommendations to tourists or travelers with venues, restaurants, coffee shops or recreational places with higher Safety and Safety Scores. So, I used three different datasets from three unique sources to make sure I extract all the necessary features that were missing in the original [MPS Borough Level Crime data](https://data.london.gov.uk/dataset/recorded_crime_summary). \n",
    "\n",
    "b. Computing features like 'Population Density' was pivotal to extract meaningful results. So, I had used additional datasets to fill the gap that our actual crime dataset lacks to address i.e., help us extract and analyze the attributes including Population Density for each of the 32 boroughs and their respective Co-ordinates.  \n",
    "The [List of London Broughs dataset](https://en.wikipedia.org/wiki/List_of_London_boroughs) provides us with this information and features. \n",
    "\n",
    "c. I used [Foursquare API](https://developer.foursquare.com/docs/api-reference/venues/details/) which gives the full details about a venue including location, tips, and categories. We can access precise, up-to-date community-sourced venue data. Its large selection of rich and firmographic location data unlocks the potential to enhance our app or website with the ability to describe locations, analyze trends, and improve user experience.\n",
    "\n",
    "If the venue ID given is one that has been merged into another venue, the response will show data about the other venue instead of giving you an error. User authenticated calls will also receive information about who is here now. This is a Premium endpoint with access to venue's photos, tips, hours, menu, categories, recommendations, events, stats, etc.\n",
    "\n",
    "Using these 3 major datasets as the basis for our project, I started leveraging its features and attributes to address our business problem.  \n",
    "\n",
    "Some missing values which I generated to address the project's motive:  \n",
    "a. MPS Borough Level Crime dataset:  \n",
    "- `Number of Incidents` - the total count of crime incidents in each borough accordingly.  \n",
    "- I generated word cloud by sing the STOPWORDS and MASKING technique for the initial analysis and presentation of borough-level crime. This helps make sure the language is accessible to policymakers or lawmakers with a quick, visual model. Also, word clouds help generate attention for meaningful prospects including Crowdsourcing, Icebreakers, Collective feedback, Quizzing or Q&As, Summarizing, etc.  \n",
    "- `BoroughCount` - the total count of the London boroughs.  \n",
    "- I used drop method to get rid of the unnecessary columns from the dataset.  \n",
    "- `MonthlyAverage` - the average count of crime incidents in a month.  \n",
    "\n",
    "b. List of London Boroughs dataset:\n",
    "- I used BeautifulSoup to extract data from the wikipedia page.  \n",
    "`source = requests.get('https://en.wikipedia.org/wiki/List_of_London_boroughs').text\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "soup.encode(\"utf-8-sig\")`  \n",
    "- I converetd the dataset into DataFrame format which was originally xlm format.  \n",
    "\n",
    "`dict = {'BoroughName' : BoroughName,\n",
    "       'Population' : Population,\n",
    "       'Coordinates': Coordinates}\n",
    "info = pd.DataFrame.from_dict(dict)\n",
    "info.head()`  \n",
    "- Stripping unwanted texts from the above dataframe.  \n",
    "`info['BoroughName'] = info['BoroughName'].map(lambda x: x.rstrip(']'))\n",
    "info['BoroughName'] = info['BoroughName'].map(lambda x: x.rstrip('1234567890.'))\n",
    "info['BoroughName'] = info['BoroughName'].str.replace('note','')\n",
    "info['BoroughName'] = info['BoroughName'].map(lambda x: x.rstrip(' ['))\n",
    "info.head()`  \n",
    "- Cleaning and stripping coordinates.  \n",
    "`info[['Coordinates1','Coordinates2','Coordinates3']] = info['Coordinates'].str.split('/',expand=True)\n",
    "info.head()`  \n",
    "- Creating new columns 'Latitude' and 'Longitude' to make this dataframe consitent with MPS Borough Level Crime.  \n",
    "`info.drop(labels=['Coordinates','Coordinates1','Coordinates2'], axis=1,inplace = True)\n",
    "info[['Latitude','Longitude']] = info['Coordinates3'].str.split(';',expand=True)\n",
    "info.head()`  \n",
    "- Use drop and strip methods to get rid of the unnecessary texts and numbers in the Latitude and Longitude columns.  \n",
    "`info.drop(labels=['Coordinates3'], axis=1,inplace = True)\n",
    "info['Latitude'] = info['Latitude'].map(lambda x: x.rstrip(u'\\ufeff'))\n",
    "info['Latitude'] = info['Latitude'].map(lambda x: x.lstrip())\n",
    "info['Longitude'] = info['Longitude'].map(lambda x: x.rstrip(')'))\n",
    "info['Longitude'] = info['Longitude'].map(lambda x: x.rstrip('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '))\n",
    "info['Longitude'] = info['Longitude'].map(lambda x: x.rstrip(' ('))\n",
    "info['Longitude'] = info['Longitude'].map(lambda x: x.rstrip(u'\\ufeff'))\n",
    "info['Longitude'] = info['Longitude'].map(lambda x: x.lstrip())\n",
    "info['Population'] = info['Population'].str.replace(',','')\n",
    "info.head()`  \n",
    "\n",
    "c. Foursquare API data:  \n",
    "- I obtained the coordinates of London using:  \n",
    "`address = 'London, UK'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinates of London are {}, {}.'.format(latitude, longitude))`  \n",
    "- Later, I used the created a map and instantiated a feature group for the incidents in the dataframe using Folium.  \n",
    "\n",
    "**One Hot Encoding and KMeans Custering:**  \n",
    "- For class label distribution, I used 'One Hot Encoding' technique to generate the features and ;abe;s in the numerical format for Clustering Analysis. On ehot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. It allows the representation of categorical data to be more expressive.  \n",
    "- The top 10 venues were extracted after performing One Hot Encoding and K-Means Clustering on the dataframe.  \n",
    "- Then, I generated a folium map based on the borough rankings obtained form the clustering results.  \n",
    "- Finally, all the resulting clusters were segmented and classiffied based on its available characteristics to meet the requirements of the target audience.  \n",
    "\n",
    "**Final Clusters and its Characteristics:**  \n",
    "Based on the results obtained, I classified the clusters, by assessing its top characteristics, for different group of audiences accordingly. This helps small businesses start or establish a strong base to attrack tourists, visitors or neighborhood residents with less proximity for crime:\n",
    "1. Cluster 0: Cluster 0 can be categorized as a 'Recreational Area' with gyms, parks, pools and golf courses as popular places.  \n",
    "2. Cluster 1: Cluster 1 can be categorized as 'Live & Electric Neighborhood' with many pubs and restaurants.  \n",
    "3. Cluster 2: Cluster 2 can be categorized as 'Shopping District' with many coffee shops and clothing stores.  \n",
    "4. Cluster 3: Cluster 3 can be ategorized as 'Peaceful & Quiet Neighborhood' with cafe, bus stop, and yoga studio as top venues.  \n",
    "5. Cluster 4: Cluster 4 can be categorized as 'Traveller or Tourist Destination' with bed & breakfast, hotels, and airport.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and techniques used in the project are thoroughly discussed and properly justified based on the characteristics of the problem.\n",
    "\n",
    "**What is K-Means Clustering**  \n",
    "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity. The results of the K-means clustering algorithm are:  \n",
    "a. The centroids of the K clusters, which can be used to label new data  \n",
    "b. Labels for the training data (each data point is assigned to a single cluster)  \n",
    "\n",
    "**Algorithm**  \n",
    "The Κ-means clustering algorithm uses iterative refinement to produce a final result. The algorithm inputs are the number of clusters Κ and the data set. The data set is a collection of features for each data point. The algorithms starts with initial estimates for the Κ centroids, which can either be randomly generated or randomly selected from the data set. The algorithm then iterates between two steps:\n",
    "\n",
    "1. Data assigment step:\n",
    "\n",
    "Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, based on the squared Euclidean distance. More formally, if ci is the collection of centroids in set C, then each data point x is assigned to a cluster based on\n",
    "\n",
    "$$\\underset{c_i \\in C}{\\arg\\min} \\; dist(c_i,x)^2$$\n",
    "\n",
    "where dist( · ) is the standard (L2) Euclidean distance. Let the set of data point assignments for each ith cluster centroid be Si.\n",
    "\n",
    "2. Centroid update step:\n",
    "\n",
    "In this step, the centroids are recomputed. This is done by taking the mean of all data points assigned to that centroid's cluster.\n",
    "\n",
    "$$c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i x_i}$$\n",
    "\n",
    "The algorithm iterates between steps one and two until a stopping criteria is met (i.e., no data points change clusters, the sum of the distances is minimized, or some maximum number of iterations is reached).\n",
    "\n",
    "This algorithm is guaranteed to converge to a result. The result may be a local optimum (i.e. not necessarily the best possible outcome), meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome.\n",
    "\n",
    "**Choosing the value of 'K'**  \n",
    "The algorithm described above finds the clusters and data set labels for a particular pre-chosen K. To find the number of clusters in the data, the user needs to run the K-means clustering algorithm for a range of K values and compare the results. In general, there is no method for determining exact value of K, but an accurate estimate can be obtained using the following techniques.\n",
    "\n",
    "One of the metrics that is commonly used to compare results across different values of K is the mean distance between data points and their cluster centroid. Since increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points. Thus, this metric cannot be used as the sole target. Instead, mean distance to the centroid as a function of K is plotted and the \"elbow point,\" where the rate of decrease sharply shifts, can be used to roughly determine K.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student clearly defines a benchmark result or threshold for comparing performances of solutions obtained.\n",
    "\n",
    "One of the benchmarks I have evaluated and made significant imporvements over is the '[Predicting London Crime Rates Using Machine Learning](https://blog.dataiku.com/predicting-london-crime-rates-using-machine-learning)' project. This project has used the same MPS London Crime Data to try to make machine learning crime predictions.  \n",
    "This project attampts to make two different predictive models: crime month-by-month at the LSOA level and crime type (whether burglary, bicycle theft, arson, etc.) month-by-month at the LSOA level. LSOA is a census area containing 1,000 to 3,000 people.  \n",
    "It also has made efforts to enrich the dataset with various open data sources, added the police station coordinates, added post codes, and inputted POIs and the LSOA statistics.  \n",
    "\n",
    "**Some of the potential shortcomings in this project:**  \n",
    "- Based on the datasets used, predictive model results, type of plugins used, I believe that this proejct only addresses the factors like 'Crime' and 'Type of crime' on a monthly basis in its final results.  \n",
    "- The predictive model generated here is leaned more towards the Data Analytics approach as opposed to addressing major factors influencing the crime using an algorithm.  \n",
    "- This project only addresses the 'obvious'. It doesn't necessarily provide insights into the reasoning and neighborhoods with feasible recommendations and suggestions to the policymakers, visitors, or small businesses.  \n",
    "- The correlation matrix generated is of no/little use since it addresses the already obvious fact i.e., the density of POIs correlated in part with a higher number of crimes. Or to quote the author, \"I was expecting a direct relationship between the number of shops, restaurants, tourist attractions, etc. and the number of crimes committed\".  \n",
    "- The primary objective of this analysis has been the 'volume of crime' and the 'type of crime' in its final results.  \n",
    "\n",
    "**How my project helps address certain shortcomings of the prior benchmark work:**  \n",
    "1. BeautifulSoup: Using BeautifulSoup for extracting original, up-to-date wikipedia data for the borough and geography information is the most feasible approach, to the best of my knowledge. And, I have gathered these inputs after having discussed this wth at least 3 to 4 Senior ML Engineers and Data Scientists.  \n",
    "2. Use of Foursquare API: I have used Foursquare Developer API which is specifically designed for developer community which provides safe and secure way to deploy the trained model into production.  \n",
    "3. Folium Maps: Apart from the meaningful visualizations which could be easily obtained after some exploratory data analysis, data normalization and data wrangling, the use of Folium Maps in this project makes it more interactive with precise latitude and longitude geography information. Folium makes it easy to visualize data that's been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector/raster/HTML visualizations as markers on the map.  \n",
    "4. One Hot Encoding: The one hot coding step before clustering helps address the problems in categorical data.  \n",
    "5. The cluster analysis in my project includes a detailed step-by-step breakdown and analysis of characteristics of each obtained cluster. Also, there is precise reasoning behind dividing the top clusters to meet the specific target audience.  \n",
    "6. Also, the use of the third dataset i.e., Folium dataset for venue information makes this model more efficient for business needs.  \n",
    "7. The resulting metrics obtained are meaningful with absolute or numerical values like `Safety`, `Score` (Safety Score), `CrimeToPop` (Recorded crime per 1000 people), `Atmosphere`. These values make the evaluation and assessment process easier for the target audience or layman.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process for which metrics, algorithms, and techniques were implemented with the given datasets or input data has been thoroughly documented. Complications that occurred during the coding process are discussed.\n",
    "\n",
    "The metrics, algorithm, and techniques are discussed in the prior sections. And, the approach of the benchmark model is as given in this article: '[Predicting London Crime Rates Using Machine Learning](https://blog.dataiku.com/predicting-london-crime-rates-using-machine-learning)' project.  \n",
    "\n",
    "And, the potential shortcomings in its implementation strategy are detailed in the above section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process of improving upon the algorithms and techniques used is clearly documented. Both the initial and final solutions are reported, along with intermediate solutions, if necessary.\n",
    "\n",
    "The 'Algorithm' and 'How to choose the K value' sections are covered in the prior sections. Now,\n",
    "\n",
    "**Applying K-Means Clustering to the Wrangled Crime dataframe:**  \n",
    "The major steps in this process include:\n",
    "- Data Cleaning and Transformation (say, One Hot Encoding in our case)  \n",
    "- Choose the initial K value and run the algorithm  \n",
    "- Review the results  \n",
    "- Ierate over several values of K  \n",
    "\n",
    "**Feature Engineering:**  \n",
    "Feature engineering is the process of using domain knowledge to choose which data metrics to input as features into a machine learning algorithm. Feature engineering plays a key role in K-means clustering; using meaningful features that capture the variability of the data is essential for the algorithm to find all of the naturally-occurring groups.  \n",
    "\n",
    "Categorical data (i.e., category labels such as Borough Name, Venue Name, Venue Type, CrimeToPop, etc.) needs to be encoded or separated in a way that can still work with the algorithm.  \n",
    "\n",
    "Feature transformations, particularly to represent rates rather than measurements, can help to normalize the data. Based on the cluster results and inferences obtained, the top 10 venues and top 5 clusters were generated by assessing the characteristics of each venue type, location, and borough.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process of improving upon the algorithms and techniques used is clearly documented. Both the initial and final solutions are reported, along with intermediate solutions, if necessary.\n",
    "\n",
    "**Alternatives:**  \n",
    "\n",
    "One way to address the potential shortcoming in this project is to try and experiment with the alternatives to KMeans Clusetring algorithm.  \n",
    "\n",
    "A number of [alternative clustering algorithms](https://blogs.oracle.com/datascience/when-k-means-clustering-fails%3a-alternatives-for-segmenting-noisy-data) exist including DBScan, spectral clustering, and modeling with Gaussian mixtures. A dimensionality reduction technique, such as principal component analysis, can be used to separate groups of patterns in data. You can read more about alternatives to K-means [in this post](https://blogs.oracle.com/datascience/when-k-means-clustering-fails%3a-alternatives-for-segmenting-noisy-data).\n",
    "\n",
    "One possible outcome is that there are no organic clusters in the data; instead, all of the data fall along the continuous feature ranges within one single group. In this case, you may need to revisit the data features to see if different measurements need to be included or a feature transformation would better represent the variability in the data. In addition, you may want to impose categories or labels based on domain knowledge and modify your analysis approach.\n",
    "\n",
    "For more information on K-means clustering, visit the [scikit learn site](https://scikit-learn.org/stable/modules/clustering.html#k-means).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final model’s qualities—such as parameters—are evaluated in detail. Some type of analysis is used to validate the robustness of the model’s solution.\n",
    "\n",
    "The benchmark model doesn't necessarily produce results that are meaningful (apart from the pre-existing metrics thata alread exist in the dataset). So, I don't have solid results from the benchmark that I could compare my model with. Having said that, following are some of the metrics and resulting clusters obtained from my model:  \n",
    "![top 5 venues_1](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/images/top%205%20venues_1.JPG)  \n",
    "\n",
    "![top 5 venues_2](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/images/top%205%20venues_2.JPG)  \n",
    "\n",
    "![Safety Score](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/images/safety%20score.JPG)  \n",
    "\n",
    "Please find the detailed results in the [Project Report (PDF)](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/The%20Battle%20of%20Neighborhoods%20(Week%202)_Report.pdf) and [Project Presentation (PDF)](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/The_Battle_of_Neighborhoods_(Week_2)_Presentation.pdf) files.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final results are compared to the benchmark result or threshold with some type of statistical analysis. Justification is made as to whether the final model and solution is significant enough to have adequately solved the problem.\n",
    "\n",
    "In addition to the results mentioned in the cell above, I am attaching the detailed lst of top 5 clusters obtained based on its unique characteristics. \n",
    "Please find all the relevant clutser results (Cluster 0 through Cluster 4) in the following presentation PDF file:  \n",
    "[Segmenting and Clustering of London Boroughs](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/The_Battle_of_Neighborhoods_(Week_2)_Presentation.pdf)  \n",
    "\n",
    "After having evaluated these results with some of the mentors and peers from the IBM Watson AI, IBM Developer Community, and Coursera community, I am certain that this model could potentially be used on Foursqaure API for its use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Conclusion, Reflection, Improvements & Discussion, Limitations sections are discussed in the jupyter notebook version of this project.  \n",
    "\n",
    "And, the necessary visualizations are included in the [Project Report (PDF)](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/The%20Battle%20of%20Neighborhoods%20(Week%202)_Report.pdf) and [Project Notebook (ipynb)](https://github.com/SandeepAswathnarayana/professional-certificate-programs/blob/master/coursera/IBM%20Data%20Science%20Professional%20Certificate/Applied%20Data%20Science%20Capstone/Final%20Capstone%20Project/The_Battle_of_Neighborhoods_(Week_2).ipynb) files of this project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
