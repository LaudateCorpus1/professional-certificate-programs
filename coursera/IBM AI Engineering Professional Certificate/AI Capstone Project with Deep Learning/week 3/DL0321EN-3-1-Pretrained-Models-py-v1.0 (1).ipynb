{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item31'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Import Libraries and Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item32'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-14 17:43:53--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\n",
      "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
      "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 261482368 (249M) [application/zip]\n",
      "Saving to: ‘concrete_data_week3.zip.4’\n",
      "\n",
      "concrete_data_week3 100%[===================>] 249.37M  13.4MB/s    in 21s     \n",
      "\n",
      "2020-03-14 17:44:15 (12.0 MB/s) - ‘concrete_data_week3.zip.4’ saved [261482368/261482368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50*** error. So please **DO NOT DO IT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item33'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Define Global Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item34'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id='item35'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Build, Compile and Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f39066222b0>,\n",
       " <keras.layers.core.Dense at 0x7f3974043940>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f3974043a58>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f3974043908>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3974057630>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3974057b00>,\n",
       " <keras.layers.core.Activation at 0x7f3974057710>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f39537c2d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39507b1fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39705d8c88>,\n",
       " <keras.layers.core.Activation at 0x7f39705b5e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3970504f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3970573cc0>,\n",
       " <keras.layers.core.Activation at 0x7f39704dbeb8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39507526a0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39506eb048>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f395073e7b8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3950340ef0>,\n",
       " <keras.layers.merge.Add at 0x7f395032df60>,\n",
       " <keras.layers.core.Activation at 0x7f395023eb70>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f395023e9b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3950272320>,\n",
       " <keras.layers.core.Activation at 0x7f395017bc50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f395016acc0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f395015bdd8>,\n",
       " <keras.layers.core.Activation at 0x7f39500c3668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c7c0400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c77fc50>,\n",
       " <keras.layers.merge.Add at 0x7f392c798080>,\n",
       " <keras.layers.core.Activation at 0x7f392c733c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c733ac8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c6d92b0>,\n",
       " <keras.layers.core.Activation at 0x7f392c661d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c5d6550>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c598eb8>,\n",
       " <keras.layers.core.Activation at 0x7f392c5ae1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c4dac88>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c4cbda0>,\n",
       " <keras.layers.merge.Add at 0x7f392c4ae630>,\n",
       " <keras.layers.core.Activation at 0x7f392c3db518>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c3db358>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c3aaa90>,\n",
       " <keras.layers.core.Activation at 0x7f39537c7e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c2c7f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c2a29e8>,\n",
       " <keras.layers.core.Activation at 0x7f392c2005c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c1dba20>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f392c0c9278>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c1ceb38>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f392c0dec88>,\n",
       " <keras.layers.merge.Add at 0x7f392c052518>,\n",
       " <keras.layers.core.Activation at 0x7f3914782da0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3914782f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3914700710>,\n",
       " <keras.layers.core.Activation at 0x7f39146e0828>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f391464a390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3914606c50>,\n",
       " <keras.layers.core.Activation at 0x7f39145e5780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f391454ac18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f391453dd30>,\n",
       " <keras.layers.merge.Add at 0x7f39144e6e48>,\n",
       " <keras.layers.core.Activation at 0x7f39144485c0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3914448400>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f391441db70>,\n",
       " <keras.layers.core.Activation at 0x7f3914379c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f391435ef60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3914353cf8>,\n",
       " <keras.layers.core.Activation at 0x7f39142a1390>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f391426f7f0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f391425e908>,\n",
       " <keras.layers.merge.Add at 0x7f3914191ba8>,\n",
       " <keras.layers.core.Activation at 0x7f3914160dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3914160fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39140ce748>,\n",
       " <keras.layers.core.Activation at 0x7f39140ad860>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c7cbb38>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c7bcc50>,\n",
       " <keras.layers.core.Activation at 0x7f390c76bd68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c6df390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c6a0b38>,\n",
       " <keras.layers.merge.Add at 0x7f390c5fd7b8>,\n",
       " <keras.layers.core.Activation at 0x7f390c5d1d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c5d1b70>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c5b8ba8>,\n",
       " <keras.layers.core.Activation at 0x7f390c526080>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c4f2710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c4de828>,\n",
       " <keras.layers.core.Activation at 0x7f390c410ac8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c3f7f60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c338390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c3e3cf8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c2f8908>,\n",
       " <keras.layers.merge.Add at 0x7f390c263a58>,\n",
       " <keras.layers.core.Activation at 0x7f390c1f8a90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c1f88d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390c1ad240>,\n",
       " <keras.layers.core.Activation at 0x7f390c0e0588>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390c09fd68>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907fc5978>,\n",
       " <keras.layers.core.Activation at 0x7f3907fa4518>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907f01978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907f71a90>,\n",
       " <keras.layers.merge.Add at 0x7f3907ea0d68>,\n",
       " <keras.layers.core.Activation at 0x7f3907e6ff28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907e6ff98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907ddc8d0>,\n",
       " <keras.layers.core.Activation at 0x7f3907db4dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907d1ce10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907cf3940>,\n",
       " <keras.layers.core.Activation at 0x7f3907c514e0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907c2e940>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907c1ea58>,\n",
       " <keras.layers.merge.Add at 0x7f3907b4fcf8>,\n",
       " <keras.layers.core.Activation at 0x7f3907b1cf60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907b1ceb8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907a8c898>,\n",
       " <keras.layers.core.Activation at 0x7f3907a64da0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39079c5fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39079a07f0>,\n",
       " <keras.layers.core.Activation at 0x7f390797c4a8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39078da908>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39078caa20>,\n",
       " <keras.layers.merge.Add at 0x7f390787bcf8>,\n",
       " <keras.layers.core.Activation at 0x7f39077c8f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39077c8e80>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39077b5860>,\n",
       " <keras.layers.core.Activation at 0x7f3907711d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39076f8f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39076be7b8>,\n",
       " <keras.layers.core.Activation at 0x7f390762f470>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f390758a8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39075f99e8>,\n",
       " <keras.layers.merge.Add at 0x7f3907527c88>,\n",
       " <keras.layers.core.Activation at 0x7f39074f8f28>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39074f8f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907467828>,\n",
       " <keras.layers.core.Activation at 0x7f390743dd30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39073a5f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f390737c780>,\n",
       " <keras.layers.core.Activation at 0x7f39072d9438>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39072b3898>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39072a29b0>,\n",
       " <keras.layers.merge.Add at 0x7f39071d9c50>,\n",
       " <keras.layers.core.Activation at 0x7f39071a5f60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39071a5f28>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39071147f0>,\n",
       " <keras.layers.core.Activation at 0x7f39070eecf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3907052fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3907029748>,\n",
       " <keras.layers.core.Activation at 0x7f3906f95400>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906f62860>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906e83c18>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906f52978>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906e54f28>,\n",
       " <keras.layers.merge.Add at 0x7f3906dd8358>,\n",
       " <keras.layers.core.Activation at 0x7f3906d73550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906d79240>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906d2f940>,\n",
       " <keras.layers.core.Activation at 0x7f3906ca7e48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906c149e8>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906c05b00>,\n",
       " <keras.layers.core.Activation at 0x7f3906bb2d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906b17f98>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906af0ac8>,\n",
       " <keras.layers.merge.Add at 0x7f3906a4d668>,\n",
       " <keras.layers.core.Activation at 0x7f3906a1fbe0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906a1fa20>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39069c4208>,\n",
       " <keras.layers.core.Activation at 0x7f39069766d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f39068c49b0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f3906934ac8>,\n",
       " <keras.layers.core.Activation at 0x7f3906866d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f3906832f60>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f39067a3a90>,\n",
       " <keras.layers.merge.Add at 0x7f390677c630>,\n",
       " <keras.layers.core.Activation at 0x7f39066cdba8>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f39066cd9e8>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f39066b5cf8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next we compile our model using the **adam** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "165/301 [===============>..............] - ETA: 57:52 - loss: 0.0693 - acc: 0.9793"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
